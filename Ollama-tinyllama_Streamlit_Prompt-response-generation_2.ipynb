{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acc3116-d467-4546-a92c-eb60ea4f8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddf2944-0f1b-4731-86e2-468ef953a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ef66ba-84fe-4749-9070-6b036d3493ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4735fa8-d80e-4be6-8d0a-ace2973d1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"tinyllama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6d5580-74e7-4e99-bf3e-31e55a528121",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70f765b-be7e-46ae-9900-a811e6f1a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is large language model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f781125-6c26-42f3-9700-4d86957eb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a Large Language Model?\n",
      "\n",
      "Step 1: Understand the Problem Statement\n",
      "Large language models (LLMs) are powerful pretrained language models that can perform translation, summarization, and other language-related tasks on large amounts of preprocessed text data. They have been increasingly used for a wide range of applications in natural language processing (NLP), such as chatbots, Q&A systems, summarization engines, and translation.\n",
      "\n",
      "Step 2: Choose the Appropriate Model for the Task\n",
      "The choice of LLM depends on several factors, including the specific application's requirements and available data. Some popular models include GPT-3 (Generative Pretrained Transformer), BART (Bidirectional Attention Recurrent Transformers), and RoBERTa (Rotated Bottom-Up Attention). \n",
      "\n",
      "Step 3: Train the Model on the Appropriate Dataset\n",
      "Training LLMs requires a significant amount of data, which can be difficult to gather for certain tasks. For example, if the task involves summarizing text, training a GPT-3 model on a large dataset containing summaries of news articles and blog posts is recommended. \n",
      "\n",
      "Step 4: Fine-tune the Model on Specific Data\n",
      "Once LLMs have been trained on appropriate datasets, fine-tuning can be done on specific data to improve their performance. For example, finetuning GPT-3 on a dataset of news articles for summarization can help improve the model's ability to generate relevant and accurate summaries. \n",
      "\n",
      "Step 5: Deploy the Model in Production Settings\n",
      "Once the LLM has been fine-tuned, it can be deployed in production settings. This requires careful consideration of the specific requirements of the application, such as how much data the system needs to process, the size and complexity of the dataset, and the scale of the system's infrastructure. \n",
      "\n",
      "In conclusion, selecting the right LLM for a specific task involves considering several factors, including the available data, the application requirements, and the desired performance level. By following these steps, you can ensure that your LLM is optimized for your specific use case and can be deployed in production settings effectively.\n"
     ]
    }
   ],
   "source": [
    "if question:\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowkernel",
   "language": "python",
   "name": "tensorflowkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
